# TEMPLATE: models/_template.yaml
# Copy this template to create new model configurations
name: "ModelName-Size"
hf_model_id: "org/model-name"  # Required: HuggingFace model ID
parameters: 1000000000  # Required: Exact parameter count (must be â‰¤3B)
architecture: "transformer"  # e.g., transformer, llama, mistral, phi
context_length: 2048  # Maximum context window
languages: ["en"]  # List of supported languages (ISO 639-1 codes)

# Quantization variants (optional, but recommended)
quantization:
  - type: "none"  # Always include FP16 baseline
  - type: "gptq"  # GPU-optimized 4-bit
    bits: 4
    hf_model_id: "org/model-name-gptq"  # Optional: if separate repo
  - type: "awq"  # Activation-aware 4-bit
    bits: 4
    hf_model_id: "org/model-name-awq"
  - type: "gguf"  # CPU-optimized
    quant: "Q4_K_M"  # Q4_K_M, Q5_K_M, Q8_0, etc.
    hf_model_id: "org/model-name-gguf"

# Metadata
tags: ["tag1", "tag2"]  # e.g., multilingual, coding-focused, edge-friendly
license: "Apache-2.0"  # Must be open-source (Apache 2.0, MIT, Llama, etc.)
release_date: "2024-01"  # YYYY-MM format
description: "Brief description of the model (1-2 sentences)"

# Optional fields
training_data: "Description of training corpus"
notes: "Any additional relevant information"
paper: "https://arxiv.org/abs/..."  # Link to paper if available
blog_post: "https://..."  # Link to announcement blog post

---
# EXAMPLE: Advanced configuration
# models/advanced-example.yaml
name: "MultiModal-SLM-1.5B"
hf_model_id: "org/multimodal-slm-1.5b"
parameters: 1500000000
architecture: "transformer-multimodal"
context_length: 8192
languages: ["en", "zh", "es", "fr", "de"]

# Multiple quantization variants
quantization:
  - type: "none"
  - type: "gptq"
    bits: 4
    group_size: 128
    desc_act: true
  - type: "gptq"
    bits: 3
    group_size: 128
  - type: "awq"
    bits: 4
    zero_point: true
  - type: "gguf"
    quant: "Q4_K_M"
  - type: "gguf"
    quant: "Q5_K_M"
  - type: "gguf"
    quant: "Q8_0"

# Extensive metadata
tags: [
  "multilingual",
  "multimodal",
  "vision-language",
  "edge-optimized",
  "mobile-ready"
]
license: "Apache-2.0"
release_date: "2024-12"
description: "Advanced multimodal small language model supporting text and vision"
training_data: "Mixed corpus: SlimPajama (text), CC3M (image-text pairs)"
notes: |
  This model supports both text-only and vision-language tasks.
  Image encoder: CLIP ViT-B/16
  Optimal for applications requiring visual understanding on edge devices
paper: "https://arxiv.org/abs/2401.12345"
blog_post: "https://blog.org/multimodal-slm"
demo: "https://huggingface.co/spaces/org/multimodal-demo"

# Hardware requirements (optional)
requirements:
  min_vram_gb: 2.0  # Minimum VRAM for FP16
  min_ram_gb: 4.0  # Minimum RAM for GGUF Q4_K_M
  recommended_device: "NVIDIA T4 or better"

# Performance hints (optional)
performance:
  throughput_fp16_t4: 145  # tokens/sec on T4
  throughput_q4_cpu: 28  # tokens/sec on CPU
  latency_ttft_ms: 85  # Time to first token