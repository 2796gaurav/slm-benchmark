# ⚡ SLM Benchmark: Technical Architecture & Methodology Report

## 1. Executive Summary

This document provides a comprehensive technical deep-dive into the SLM Benchmark platform. It covers the codebase architecture, benchmarking engine logic, scoring algorithms, automation workflows, and the model submission process. The platform is designed to be a definitive, unbiased, and reproducible standard for evaluating Small Language Models (1M-3B parameters).

## 2. Codebase Architecture

The project is structured as a hybrid Python-based evaluation engine and a static web interface, orchestrated by GitHub Actions.

### 2.1. Directory Structure

```
slm-benchmark/
├── .github/
│   ├── workflows/           # CI/CD pipelines (submission, publishing)
│   └── scripts/             # Internal automation (permissions, cleanup)
├── benchmarks/
│   ├── evaluation/          # Core evaluation logic
│   │   ├── run_benchmark.py # Main entry point
│   │   ├── edge_benchmark.py# Performance metrics (latency, memory)
│   │   └── safety_eval.py   # Safety & bias evaluation
│   └── configs/             # Task-specific configurations
├── models/
│   ├── registry.json        # Database of approved/benchmarked models
│   └── submissions/         # Queue for pending model YAMLs
├── results/
│   ├── raw/                 # Detailed JSON outputs from runs
│   └── processed/           # Aggregated data for the frontend
├── scripts/                 # Utility scripts (report generation, website updates)
└── website/                 # Static HTML/JS frontend
    ├── assets/
    │   ├── js/              # Client-side logic (sorting, filtering)
    │   └── data/            # JSON data sources
    └── ...html              # Web pages
```

### 2.2. Key Components

1.  **Benchmarking Engine (`benchmarks/evaluation/`)**: A Python application leveraging `lm-evaluation-harness`, `transformers`, and `llama.cpp` to execute standardized tests.
2.  **Orchestrator (`.github/workflows/`)**: A set of event-driven workflows that automate the lifecycle of a model submission from PR to Leaderboard.
3.  **Frontend (`website/`)**: A zero-dependency static site that consumes JSON data generated by the backend to display interactive leaderboards.

## 3. Benchmarking Methodology

The core philosophy of SLM Benchmark is **determinism** and **reproducibility**. Every benchmark run is executed in a controlled environment with fixed random seeds.

### 3.1. Evaluation Engine Logic

The `SLMBenchmark` class in `benchmarks/evaluation/run_benchmark.py` is the central controller. It performs the following sequence:

1.  **Initialization**:
    -   Sets global random seeds (`torch.manual_seed(42)`) for reproducibility.
    -   Configures the environment (detects CUDA/CPU).
    -   Captures system metadata (hardware specs, library versions).

2.  **Model Loading**:
    -   Supports two concurrent backends:
        -   **Transformers**: For standard PyTorch FP16/BF16 evaluation.
        -   **Llama.cpp**: For quantized (GGUF) edge-optimized evaluation.

3.  **Task Execution**:
    -   Iterates through defined categories (Reasoning, Coding, etc.).
    -   Calls `evaluator.simple_evaluate` from `lm-evaluation-harness` for academic benchmarks.
    -   Calls custom `EdgeBenchmark` and `SafetyEvaluator` classes for specialized metrics.

### 3.2. Evaluation Categories & Metrics

The benchmark covers 360 degrees of model capability:

| Category | Weight | Description | Key Tasks |
|----------|--------|-------------|-----------|
| **Reasoning** | 30% | Logical deduction and world knowledge | MMLU, ARC-Challenge, HellaSwag, TruthfulQA |
| **Coding** | 20% | Code generation and understanding | HumanEval, MBPP |
| **Math** | 15% | Mathematical problem solving | GSM8K, MATH |
| **Language** | 15% | Linguistic nuance and comprehension | BoolQ, PIQA, WinoGrande |
| **Edge** | 10% | On-device efficiency | Latency (ms/token), Throughput (t/s), Memory (GB) |
| **Safety** | 10% | Output verification and bias detection | Toxicity scoring, Bias metrics |

### 3.3. Scoring Logic & Formulas

The **Aggregate Score** is a weighted average of normalized category scores.

$$
Score_{final} = \sum_{c \in Categories} (Score_c \times Weight_c)
$$

Where $Score_c$ is the average accuracy (0-100) of all tasks within that category.

**Example Calculation:**
-   **Reasoning**: 65.0 (acc) * 0.30 = 19.5
-   **Coding**: 40.0 (pass@1) * 0.20 = 8.0
-   ...
-   **Total**: 71.8

*Note: Edge metrics are normalized such that lower latency/memory yields a higher score.*

## 4. Automation & Submission Workflow

The platform uses a "ChatOps" style workflow powered by GitHub Actions, defined in `.github/workflows/02-submission-workflow.yml`.

### 4.1. The Submission Pipeline

1.  **User Action**: A user forks the repo and creates a Pull Request (PR) adding a YAML file to `models/submissions/`.
2.  **Trigger**: The workflow listens for specific comments on the PR.
3.  **Validation (Bot)**: Checks if the YAML structure is valid and the model is accessible on Hugging Face.

### 4.2. Command Hierarchy

-   `/test-benchmark`:
    -   **Permissions**: Restricted to maintainers.
    -   **Action**: Spins up a runner, installs dependencies, and executes `run_benchmark.py` against the submitted model.
    -   **Output**: Posts a comment on the PR with a summary table and raw JSON artifacts.

-   `/push-results`:
    -   **Permissions**: Restricted to maintainers.
    -   **Action**:
        1.  Downloads results from the benchmark run.
        2.  Updates `models/registry.json`.
        3.  Updates `website/assets/data/leaderboard.json`.
        4.  Archives raw results.
        5.  Merges the PR.
        6.  Commits and pushes changes to `main`.
    -   **Result**: The website automatically redeploys, and the leaderboard is updated.

## 5. Technical Requirements & Environment

To ensure fair comparison, all models are evaluated on standardized hardware profiles.

-   **Backend**: `benchmarks/evaluation/run_benchmark.py`
-   **Hardware Profile**:
    -   **GPU**: NVIDIA T4 (Active) or RTX 4090 (Reference)
    -   **RAM**: 16GB+ System RAM
    -   **CPU**: AVX2 supported processor
-   **Dependencies**:
    -   `torch` (PyTorch)
    -   `lm_eval` (Language Model Evaluation Harness)
    -   `llama_cpp` (for GGUF support)
    -   `transformers`, `datasets`, `accelerate`

## 6. Frontend Architecture

The website is a static Single Page Application (SPA)-like site hosted on GitHub Pages.

-   **Data Source**: Reads from `website/assets/data/leaderboard.json`. This file is statically generated by the CI pipeline, ensuring fast load times and no backend database queries.
-   **Logic**: `website/assets/js/leaderboard.js` handles:
    -   **Parsing**: Converts parameter strings ("1.5B") into sortable numbers.
    -   **Rendering**: Dynamic DOM manipulation to build the leaderboard table.
    -   **Filtering**: Client-side filtering for searching, categories, and quantization types.
-   **Design**: Uses vanilla CSS Variables for theming (Light/Dark mode) and responsive Grid/Flexbox layouts.

## 7. Submission Guide (Technical)

To submit a model, create a file at `models/submissions/<model_name>.yaml`:

```yaml
model:
  name: "MyModel-1B"
  hf_repo: "username/mymodel-1b"
  parameters: "1.1B"
  quantizations:
    - name: "FP16"
      format: "safetensors"
  categories: ["reasoning", "coding"]
```

**Testing Methodology for Users:**
Users can run the benchmark locally before submitting:
```bash
python benchmarks/evaluation/run_benchmark.py \
  --submission-file models/submissions/mymodel.yaml \
  --output-dir local_results/ \
  --limit 5  # Run a quick smoke test
```

## 8. Safety & Security

-   **Code Execution**: Coding benchmarks (HumanEval) are run with `confirm_run_unsafe_code=True` inside isolated container environments to prevent system compromise.
-   **Rate Limiting**: GitHub Actions concurrency groups prevent resource exhaustion.
-   **Audit Trail**: Every benchmark run ID and result is logged in the PR comments and the git history.
