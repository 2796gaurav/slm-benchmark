{
  "models": [
    {
      "id": "phi-3.5-mini-3.8b",
      "name": "Phi-3.5-Mini-3.8B",
      "family": "Phi",
      "hf_repo": "microsoft/Phi-3.5-mini-instruct",
      "parameters": "3.8B",
      "license": "MIT",
      "context_window": 131072,
      "architecture": "Phi3",
      "use_cases": {
        "rag": {
          "score": 82.2,
          "benchmarks": {
            "niah": 82.1,
            "ruler": 79.3,
            "frames": 85.2
          },
          "recommended": true
        },
        "function_calling": {
          "score": 66.80000000000001,
          "benchmarks": {
            "bfcl_v2": 68.4,
            "api_accuracy": 65.2
          },
          "recommended": false
        },
        "coding": {
          "score": 71.80000000000001,
          "benchmarks": {
            "humaneval": 68.2,
            "mbpp": 75.4
          },
          "recommended": true
        },
        "reasoning": {
          "score": 75.60000000000001,
          "benchmarks": {
            "mmlu": 72.1,
            "arc_challenge": 78.4,
            "hellaswag": 76.3
          },
          "recommended": true
        },
        "guardrails": {
          "score": 81.2,
          "benchmarks": {
            "toxicity": 85.3,
            "bias": 78.1,
            "truthfulness": 80.2
          },
          "recommended": true
        }
      },
      "domain_scores": {
        "general": 75.6,
        "finance": null,
        "healthcare": null,
        "legal": null
      },
      "performance": {
        "hardware": "GitHub Actions (2-core CPU)",
        "quantizations": {
          "fp16": {
            "size_gb": 7.6,
            "ram_gb": 9.8,
            "tps_output": 9.8,
            "ttft_ms": 312
          },
          "q8": {
            "size_gb": 3.9,
            "ram_gb": 7.8,
            "tps_output": 19.4,
            "ttft_ms": 245
          },
          "q4": {
            "size_gb": 2.1,
            "ram_gb": 5.2,
            "tps_output": 32.1,
            "ttft_ms": 198
          }
        }
      },
      "safety": {
        "guardrails_compatible": true,
        "hallucination_rate": 2.8,
        "bias_score": "A-",
        "toxicity_rate": 0.9
      },
      "compliance": {
        "hipaa": false,
        "gdpr": true,
        "finra": false
      },
      "tags": [
        "reasoning",
        "instruct-tuned",
        "long-context",
        "128k-context"
      ],
      "deployment_targets": [
        "jetson_orin",
        "16gb_laptop",
        "32gb_workstation"
      ],
      "best_for": [
        "Rag",
        "Coding",
        "Reasoning",
        "Guardrails"
      ],
      "not_for": [],
      "aggregate_score": 75.76,
      "date_added": "2026-01-14",
      "submitted_by": "slm-marketplace",
      "rank": 1
    },
    {
      "id": "qwen2.5-3b-instruct",
      "name": "Qwen2.5-3B-Instruct",
      "family": "Qwen",
      "hf_repo": "Qwen/Qwen2.5-3B-Instruct",
      "parameters": "3B",
      "license": "Apache-2.0",
      "context_window": 32768,
      "architecture": "Qwen2",
      "use_cases": {
        "rag": {
          "score": 83.73333333333333,
          "benchmarks": {
            "niah": 87.3,
            "ruler": 84.1,
            "frames": 79.8
          },
          "recommended": true
        },
        "function_calling": {
          "score": 70.25,
          "benchmarks": {
            "bfcl_v2": 72.1,
            "api_accuracy": 68.4
          },
          "recommended": true
        },
        "coding": {
          "score": 65.30000000000001,
          "benchmarks": {
            "humaneval": 58.7,
            "mbpp": 71.9
          },
          "recommended": false
        },
        "reasoning": {
          "score": 68.2,
          "benchmarks": {
            "mmlu": 65.4,
            "arc_challenge": 72.3,
            "hellaswag": 66.9
          },
          "recommended": false
        },
        "guardrails": {
          "score": 78.5,
          "benchmarks": {
            "toxicity": 82.1,
            "bias": 75.3,
            "truthfulness": 78.1
          },
          "recommended": true
        }
      },
      "domain_scores": {
        "general": 68.2,
        "finance": null,
        "healthcare": null,
        "legal": null
      },
      "performance": {
        "hardware": "GitHub Actions (2-core CPU)",
        "quantizations": {
          "fp16": {
            "size_gb": 6.0,
            "ram_gb": 8.2,
            "tps_output": 12.1,
            "ttft_ms": 234
          },
          "q8": {
            "size_gb": 3.1,
            "ram_gb": 6.2,
            "tps_output": 24.1,
            "ttft_ms": 187
          },
          "q4": {
            "size_gb": 1.7,
            "ram_gb": 4.1,
            "tps_output": 38.7,
            "ttft_ms": 156
          }
        }
      },
      "safety": {
        "guardrails_compatible": true,
        "hallucination_rate": 3.2,
        "bias_score": "B+",
        "toxicity_rate": 1.1
      },
      "compliance": {
        "hipaa": false,
        "gdpr": true,
        "finra": false
      },
      "tags": [
        "multilingual",
        "instruct-tuned",
        "edge-ready",
        "long-context"
      ],
      "deployment_targets": [
        "raspberry_pi_5",
        "jetson_orin",
        "16gb_laptop",
        "mobile_high_end"
      ],
      "best_for": [
        "Rag",
        "Function Calling",
        "Guardrails"
      ],
      "not_for": [],
      "aggregate_score": 73.6,
      "date_added": "2026-01-14",
      "submitted_by": "slm-marketplace",
      "rank": 2
    },
    {
      "id": "llama-3.2-3b-instruct",
      "name": "Llama-3.2-3B-Instruct",
      "family": "Llama",
      "hf_repo": "meta-llama/Llama-3.2-3B-Instruct",
      "parameters": "3B",
      "license": "Llama-3.2",
      "context_window": 131072,
      "architecture": "Llama3",
      "use_cases": {
        "rag": {
          "score": 79.06666666666666,
          "benchmarks": {
            "niah": 81.2,
            "ruler": 78.1,
            "frames": 77.9
          },
          "recommended": true
        },
        "function_calling": {
          "score": 73.05,
          "benchmarks": {
            "bfcl_v2": 74.8,
            "api_accuracy": 71.3
          },
          "recommended": true
        },
        "coding": {
          "score": 62.1,
          "benchmarks": {
            "humaneval": 55.3,
            "mbpp": 68.9
          },
          "recommended": false
        },
        "reasoning": {
          "score": 70.3,
          "benchmarks": {
            "mmlu": 68.2,
            "arc_challenge": 73.1,
            "hellaswag": 69.6
          },
          "recommended": true
        },
        "guardrails": {
          "score": 76.8,
          "benchmarks": {
            "toxicity": 80.2,
            "bias": 74.1,
            "truthfulness": 76.1
          },
          "recommended": true
        }
      },
      "domain_scores": {
        "general": 70.3,
        "finance": null,
        "healthcare": null,
        "legal": null
      },
      "performance": {
        "hardware": "GitHub Actions (2-core CPU)",
        "quantizations": {
          "fp16": {
            "size_gb": 6.0,
            "ram_gb": 8.0,
            "tps_output": 11.2,
            "ttft_ms": 256
          },
          "q8": {
            "size_gb": 3.0,
            "ram_gb": 6.0,
            "tps_output": 22.3,
            "ttft_ms": 198
          },
          "q4": {
            "size_gb": 1.6,
            "ram_gb": 3.8,
            "tps_output": 36.4,
            "ttft_ms": 167
          }
        }
      },
      "safety": {
        "guardrails_compatible": true,
        "hallucination_rate": 4.1,
        "bias_score": "B",
        "toxicity_rate": 1.3
      },
      "compliance": {
        "hipaa": false,
        "gdpr": true,
        "finra": false
      },
      "tags": [
        "multilingual",
        "instruct-tuned",
        "long-context",
        "function-calling"
      ],
      "deployment_targets": [
        "raspberry_pi_5",
        "jetson_orin",
        "16gb_laptop"
      ],
      "best_for": [
        "Rag",
        "Function Calling",
        "Reasoning",
        "Guardrails"
      ],
      "not_for": [],
      "aggregate_score": 72.79,
      "date_added": "2026-01-14",
      "submitted_by": "slm-marketplace",
      "rank": 3
    },
    {
      "id": "gemma-2-2.6b-it",
      "name": "Gemma-2-2.6B-IT",
      "family": "Gemma",
      "hf_repo": "google/gemma-2-2b-it",
      "parameters": "2.6B",
      "license": "Gemma",
      "context_window": 8192,
      "architecture": "Gemma2",
      "use_cases": {
        "rag": {
          "score": 74.8,
          "benchmarks": {
            "niah": 76.2,
            "ruler": 73.1,
            "frames": 75.1
          },
          "recommended": true
        },
        "function_calling": {
          "score": 63.7,
          "benchmarks": {
            "bfcl_v2": 65.3,
            "api_accuracy": 62.1
          },
          "recommended": false
        },
        "coding": {
          "score": 68.7,
          "benchmarks": {
            "humaneval": 64.2,
            "mbpp": 73.2
          },
          "recommended": false
        },
        "reasoning": {
          "score": 72.39999999999999,
          "benchmarks": {
            "mmlu": 70.1,
            "arc_challenge": 74.3,
            "hellaswag": 72.8
          },
          "recommended": true
        },
        "guardrails": {
          "score": 79.10000000000001,
          "benchmarks": {
            "toxicity": 82.3,
            "bias": 76.8,
            "truthfulness": 78.2
          },
          "recommended": true
        }
      },
      "domain_scores": {
        "general": 72.4,
        "finance": null,
        "healthcare": null,
        "legal": null
      },
      "performance": {
        "hardware": "GitHub Actions (2-core CPU)",
        "quantizations": {
          "fp16": {
            "size_gb": 5.2,
            "ram_gb": 7.0,
            "tps_output": 14.3,
            "ttft_ms": 198
          },
          "q8": {
            "size_gb": 2.6,
            "ram_gb": 5.2,
            "tps_output": 26.8,
            "ttft_ms": 156
          },
          "q4": {
            "size_gb": 1.4,
            "ram_gb": 3.4,
            "tps_output": 42.1,
            "ttft_ms": 124
          }
        }
      },
      "safety": {
        "guardrails_compatible": true,
        "hallucination_rate": 3.8,
        "bias_score": "A-",
        "toxicity_rate": 0.8
      },
      "compliance": {
        "hipaa": false,
        "gdpr": true,
        "finra": false
      },
      "tags": [
        "google",
        "instruct-tuned",
        "efficient",
        "safe"
      ],
      "deployment_targets": [
        "raspberry_pi_5",
        "jetson_orin",
        "16gb_laptop",
        "mobile_high_end"
      ],
      "best_for": [
        "Rag",
        "Reasoning",
        "Guardrails"
      ],
      "not_for": [],
      "aggregate_score": 71.71,
      "date_added": "2026-01-14",
      "submitted_by": "slm-marketplace",
      "rank": 4
    },
    {
      "id": "smollm2-1.7b-instruct",
      "name": "SmolLM2-1.7B-Instruct",
      "family": "SmolLM",
      "hf_repo": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
      "parameters": "1.7B",
      "license": "Apache-2.0",
      "context_window": 8192,
      "architecture": "SmolLM2",
      "use_cases": {
        "rag": {
          "score": 71.2,
          "benchmarks": {
            "niah": 72.4,
            "ruler": 69.8,
            "frames": 71.4
          },
          "recommended": true
        },
        "function_calling": {
          "score": 74.65,
          "benchmarks": {
            "bfcl_v2": 76.2,
            "api_accuracy": 73.1
          },
          "recommended": true
        },
        "coding": {
          "score": 58.400000000000006,
          "benchmarks": {
            "humaneval": 52.1,
            "mbpp": 64.7
          },
          "recommended": false
        },
        "reasoning": {
          "score": 62.1,
          "benchmarks": {
            "mmlu": 58.3,
            "arc_challenge": 65.2,
            "hellaswag": 62.8
          },
          "recommended": false
        },
        "guardrails": {
          "score": 72.3,
          "benchmarks": {
            "toxicity": 76.1,
            "bias": 70.2,
            "truthfulness": 70.6
          },
          "recommended": true
        }
      },
      "domain_scores": {
        "general": 62.1,
        "finance": null,
        "healthcare": null,
        "legal": null
      },
      "performance": {
        "hardware": "GitHub Actions (2-core CPU)",
        "quantizations": {
          "fp16": {
            "size_gb": 3.4,
            "ram_gb": 5.2,
            "tps_output": 18.7,
            "ttft_ms": 145
          },
          "q8": {
            "size_gb": 1.7,
            "ram_gb": 3.4,
            "tps_output": 31.7,
            "ttft_ms": 112
          },
          "q4": {
            "size_gb": 0.9,
            "ram_gb": 2.1,
            "tps_output": 48.2,
            "ttft_ms": 89
          }
        }
      },
      "safety": {
        "guardrails_compatible": true,
        "hallucination_rate": 5.2,
        "bias_score": "B",
        "toxicity_rate": 1.8
      },
      "compliance": {
        "hipaa": false,
        "gdpr": true,
        "finra": false
      },
      "tags": [
        "lightweight",
        "instruct-tuned",
        "edge-optimized",
        "fast-inference"
      ],
      "deployment_targets": [
        "raspberry_pi_4",
        "raspberry_pi_5",
        "mobile_high_end",
        "8gb_laptop"
      ],
      "best_for": [
        "Rag",
        "Function Calling",
        "Guardrails"
      ],
      "not_for": [],
      "aggregate_score": 67.86,
      "date_added": "2026-01-14",
      "submitted_by": "slm-marketplace",
      "rank": 5
    },
    {
      "id": "ministral-3b-instruct",
      "name": "Ministral-3B-Instruct",
      "family": "Ministral",
      "hf_repo": "mistralai/Ministral-3B-Instruct-2410",
      "parameters": "3B",
      "license": "Mistral-Non-Commercial",
      "context_window": 131072,
      "architecture": "Mistral",
      "use_cases": {
        "rag": {
          "score": 88.5,
          "benchmarks": {
            "niah": 91.2,
            "ruler": 85.8
          },
          "recommended": true
        },
        "coding": {
          "score": 74.2,
          "benchmarks": {
            "humaneval": 71.5,
            "mbpp": 76.9
          },
          "recommended": true
        }
      },
      "performance": {
        "hardware": "GitHub Actions (2-core CPU)",
        "quantizations": {
          "q4": {
            "size_gb": 1.8,
            "ram_gb": 4.5,
            "tps_output": 35.2,
            "ttft_ms": 178
          }
        }
      },
      "tags": [
        "mistral",
        "high-accuracy",
        "long-context"
      ],
      "aggregate_score": 33.26,
      "date_added": "2026-01-14",
      "submitted_by": "slm-marketplace",
      "rank": 6,
      "best_for": [
        "Rag",
        "Coding"
      ],
      "not_for": []
    },
    {
      "id": "doge-20m",
      "name": "Doge-20M",
      "family": "SmallDoge",
      "hf_repo": "SmallDoge/Doge-20M",
      "parameters": "N/A",
      "license": "Unknown",
      "context_window": 4096,
      "architecture": "Unknown",
      "use_cases": {
        "reasoning": {
          "score": 21.85215118789766,
          "benchmarks": {
            "arc_challenge": {
              "alias": "arc_challenge",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783,
              "acc_norm,none": 0.2,
              "acc_norm_stderr,none": 0.2
            },
            "hellaswag": {
              "alias": "hellaswag",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0,
              "acc_norm,none": 0.4,
              "acc_norm_stderr,none": 0.24494897427831783
            },
            "mmlu": {
              "acc,none": 0.21052631578947367,
              "acc_stderr,none": 0.025180175604223375,
              "alias": "mmlu"
            },
            "mmlu_humanities": {
              "acc,none": 0.23076923076923078,
              "acc_stderr,none": 0.054392829322042126,
              "alias": " - humanities"
            },
            "mmlu_formal_logic": {
              "alias": "  - formal_logic",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_high_school_european_history": {
              "alias": "  - high_school_european_history",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_high_school_us_history": {
              "alias": "  - high_school_us_history",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_high_school_world_history": {
              "alias": "  - high_school_world_history",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_international_law": {
              "alias": "  - international_law",
              "acc,none": 0.6,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_jurisprudence": {
              "alias": "  - jurisprudence",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_logical_fallacies": {
              "alias": "  - logical_fallacies",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_moral_disputes": {
              "alias": "  - moral_disputes",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_moral_scenarios": {
              "alias": "  - moral_scenarios",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_philosophy": {
              "alias": "  - philosophy",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_prehistory": {
              "alias": "  - prehistory",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_professional_law": {
              "alias": "  - professional_law",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_world_religions": {
              "alias": "  - world_religions",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_other": {
              "acc,none": 0.15384615384615385,
              "acc_stderr,none": 0.04485347611419462,
              "alias": " - other"
            },
            "mmlu_business_ethics": {
              "alias": "  - business_ethics",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_clinical_knowledge": {
              "alias": "  - clinical_knowledge",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_college_medicine": {
              "alias": "  - college_medicine",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_global_facts": {
              "alias": "  - global_facts",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_human_aging": {
              "alias": "  - human_aging",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_management": {
              "alias": "  - management",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_marketing": {
              "alias": "  - marketing",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_medical_genetics": {
              "alias": "  - medical_genetics",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_miscellaneous": {
              "alias": "  - miscellaneous",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_nutrition": {
              "alias": "  - nutrition",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_professional_accounting": {
              "alias": "  - professional_accounting",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_professional_medicine": {
              "alias": "  - professional_medicine",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_virology": {
              "alias": "  - virology",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_social_sciences": {
              "acc,none": 0.25,
              "acc_stderr,none": 0.060092521257733164,
              "alias": " - social sciences"
            },
            "mmlu_econometrics": {
              "alias": "  - econometrics",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_high_school_geography": {
              "alias": "  - high_school_geography",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_high_school_government_and_politics": {
              "alias": "  - high_school_government_and_politics",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_high_school_macroeconomics": {
              "alias": "  - high_school_macroeconomics",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_high_school_microeconomics": {
              "alias": "  - high_school_microeconomics",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_high_school_psychology": {
              "alias": "  - high_school_psychology",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_human_sexuality": {
              "alias": "  - human_sexuality",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_professional_psychology": {
              "alias": "  - professional_psychology",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_public_relations": {
              "alias": "  - public_relations",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_security_studies": {
              "alias": "  - security_studies",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_sociology": {
              "alias": "  - sociology",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_us_foreign_policy": {
              "alias": "  - us_foreign_policy",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_stem": {
              "acc,none": 0.21052631578947367,
              "acc_stderr,none": 0.04403473823863556,
              "alias": " - stem"
            },
            "mmlu_abstract_algebra": {
              "alias": "  - abstract_algebra",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_anatomy": {
              "alias": "  - anatomy",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_astronomy": {
              "alias": "  - astronomy",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_college_biology": {
              "alias": "  - college_biology",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_college_chemistry": {
              "alias": "  - college_chemistry",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_college_computer_science": {
              "alias": "  - college_computer_science",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_college_mathematics": {
              "alias": "  - college_mathematics",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_college_physics": {
              "alias": "  - college_physics",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_computer_security": {
              "alias": "  - computer_security",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_conceptual_physics": {
              "alias": "  - conceptual_physics",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_electrical_engineering": {
              "alias": "  - electrical_engineering",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_elementary_mathematics": {
              "alias": "  - elementary_mathematics",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_high_school_biology": {
              "alias": "  - high_school_biology",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_high_school_chemistry": {
              "alias": "  - high_school_chemistry",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_high_school_computer_science": {
              "alias": "  - high_school_computer_science",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_high_school_mathematics": {
              "alias": "  - high_school_mathematics",
              "acc,none": 0.2,
              "acc_stderr,none": 0.2
            },
            "mmlu_high_school_physics": {
              "alias": "  - high_school_physics",
              "acc,none": 0.4,
              "acc_stderr,none": 0.24494897427831783
            },
            "mmlu_high_school_statistics": {
              "alias": "  - high_school_statistics",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "mmlu_machine_learning": {
              "alias": "  - machine_learning",
              "acc,none": 0.0,
              "acc_stderr,none": 0.0
            },
            "truthfulqa_mc2": {
              "alias": "truthfulqa_mc2",
              "acc,none": 0.5482302559391468,
              "acc_stderr,none": 0.17523382933157705
            }
          },
          "recommended": false
        }
      },
      "domain_scores": {
        "general": 0.0,
        "finance": null,
        "healthcare": null,
        "legal": null
      },
      "performance": {
        "hardware": "GitHub Actions (2-core CPU)",
        "quantizations": {}
      },
      "safety": {
        "guardrails_compatible": true,
        "hallucination_rate": 0,
        "bias_score": "N/A",
        "toxicity_rate": 0
      },
      "compliance": {
        "hipaa": false,
        "gdpr": true,
        "finra": false
      },
      "tags": [],
      "deployment_targets": [],
      "best_for": [],
      "not_for": [
        "Reasoning"
      ],
      "aggregate_score": 5.46,
      "date_added": "2026-01-14",
      "submitted_by": "unknown",
      "rank": 7
    }
  ],
  "metadata": {
    "last_updated": "2026-01-14T21:02:17.340435",
    "total_models": 7,
    "schema_version": "2.0.0",
    "platform": "SLM Marketplace"
  }
}