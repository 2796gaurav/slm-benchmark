name: SLM Benchmark Runner

on:
  workflow_dispatch:
    inputs:
      model_config:
        description: 'Model config file (e.g., models/qwen2-0.5b.yaml)'
        required: true
        type: string
  pull_request:
    paths:
      - 'models/*.yaml'
    types: [opened, synchronize]

permissions:
  contents: write
  pull-requests: write

jobs:
  validate:
    name: Validate Model Configuration
    runs-on: ubuntu-latest
    outputs:
      model_name: ${{ steps.extract.outputs.model_name }}
      model_valid: ${{ steps.validate.outputs.valid }}
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install Dependencies
        run: |
          pip install pyyaml requests
      
      - name: Extract Model Name
        id: extract
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            CONFIG_FILE="${{ github.event.inputs.model_config }}"
          else
            # Get changed files in PR
            CONFIG_FILE=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | grep '^models/.*\.yaml$' | head -n1)
          fi
          echo "config_file=$CONFIG_FILE" >> $GITHUB_OUTPUT
          MODEL_NAME=$(basename "$CONFIG_FILE" .yaml)
          echo "model_name=$MODEL_NAME" >> $GITHUB_OUTPUT
      
      - name: Validate Model Config
        id: validate
        run: |
          python scripts/validate_model.py "${{ steps.extract.outputs.config_file }}"
          echo "valid=true" >> $GITHUB_OUTPUT

  benchmark-cpu:
    name: Run CPU Benchmark
    needs: validate
    if: needs.validate.outputs.model_valid == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y cmake build-essential
      
      - name: Install Python Dependencies
        run: |
          pip install --upgrade pip
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install -r benchmark/requirements.txt
          pip install lm-eval[openai]
      
      - name: Run Benchmark
        env:
          MODEL_NAME: ${{ needs.validate.outputs.model_name }}
        run: |
          python benchmark/run_eval.py \
            --model "models/${MODEL_NAME}.yaml" \
            --device cpu \
            --batch-size 4 \
            --tasks leaderboard \
            --output "results/${MODEL_NAME}-cpu"
      
      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-cpu
          path: results/${{ needs.validate.outputs.model_name }}-cpu/

  # Optional: Trigger GPU benchmark on external service
  benchmark-gpu:
    name: Request GPU Benchmark
    needs: validate
    if: needs.validate.outputs.model_valid == 'true' && github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Trigger Paperspace Gradient Job
        env:
          PAPERSPACE_API_KEY: ${{ secrets.PAPERSPACE_API_KEY }}
          MODEL_NAME: ${{ needs.validate.outputs.model_name }}
        run: |
          # Alternative: Use Paperspace Gradient, RunPod, or Lambda Labs API
          # For now, create a placeholder result
          echo "GPU benchmark requested for ${MODEL_NAME}"
          echo "Manual Colab run required - see docs/COLAB_GUIDE.md"

  commit-results:
    name: Commit Results to Repository
    needs: [validate, benchmark-cpu]
    runs-on: ubuntu-latest
    if: always() && needs.benchmark-cpu.result == 'success'
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Download Results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-cpu
          path: results/${{ needs.validate.outputs.model_name }}-cpu/
      
      - name: Commit Results
        env:
          MODEL_NAME: ${{ needs.validate.outputs.model_name }}
        run: |
          git config user.name "SLM Benchmark Bot"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add results/${MODEL_NAME}-cpu/
          git commit -m "Add benchmark results for ${MODEL_NAME} (CPU)" || echo "No changes to commit"
          git push

  update-leaderboard:
    name: Update Leaderboard
    needs: [commit-results]
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          ref: main
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Aggregate Results
        run: |
          python scripts/aggregate_results.py
      
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./website
          publish_branch: gh-pages
          force_orphan: true